{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2 part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create two lists edges and n, one for edges and the last to save al nodes that we find in edges\n",
    "edges = []\n",
    "n =[]\n",
    "#we import data that we need and we use them to fill previous initialized lists \n",
    "with open(\"wiki-topcats-reduced.txt\\wiki-topcats-reduced.txt\") as mytxt:\n",
    "    for line in mytxt:\n",
    "        #splitting edge to get the two nodes that form it\n",
    "        i,j= line.split()\n",
    "        #adding the nodes to n list\n",
    "        n.extend((int(i),int(j)))\n",
    "        #adding the edge to edges list\n",
    "        edges.append([int(i),int(j)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we eliminate duplicates from nodes\n",
    "nodes = set(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary with articles for each category, we take only articles in the reduced version of the graph \n",
    "and only categories with more than 3500 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dictionary\n",
    "cat_dic = {}\n",
    "with open (\"wiki-topcats-categories.txt\")as dc:\n",
    "    for line in dc:\n",
    "        #initializing a set to save articles of categories that are in the graph\n",
    "        art1 = set([])\n",
    "        #eliminating from the line the unuseful part\n",
    "        l1 = line.replace(\"Category:\",\"\")\n",
    "        #splitting name of the category from its articles\n",
    "        l2 = l1.split(\";\")\n",
    "        k = l2[0]\n",
    "        #converting articles in int\n",
    "        art = list(map(int,l2[1].split()))\n",
    "        #checking if articles are in our graph\n",
    "        for a in art :\n",
    "            if a in nodes:\n",
    "                art1.add(a) \n",
    "        #checking if the category meets the requirement and saving it into the dictionary\n",
    "        if len(art1)>3500:\n",
    "             cat_dic[k] = art1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the graph in a dictionary saving from each source node a list with end nodes of its edges\n",
    "we use defaultdict to have a list as a default attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = defaultdict(list)\n",
    "for e in edges:\n",
    "    g[e[0]].append(e[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this algorithm, derived in part from bfs,we add the distance(of the shortest path) from the starting point to each \n",
    "node that is connected to the starting point, we save thtis data in a dictionary. The function requires a starting graph in dictionary format,a starting point and the dictionary to add distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_mod(graph, start,dists):\n",
    "    #initialize a set with visited\n",
    "    visited = set([])\n",
    "    #we begin the queue with the starting point\n",
    "    queue = [start]\n",
    "    #the counter,it starts from 0\n",
    "    c = 0\n",
    "    while queue:\n",
    "        l =[]\n",
    "        #for each node at this level wee add the distance from the starting point\n",
    "        for q in queue:\n",
    "            if q not in visited:\n",
    "                #appending the distance of the shortest path to dists for the node(q) key\n",
    "                dists[q].append(c)\n",
    "                #adding q to visited\n",
    "                visited.add(q)\n",
    "                #adding successors of q to the queue\n",
    "                l.extend(graph[q])\n",
    "        queue = l\n",
    "        #we increase the counter(distance)\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a dictionary with all nodes in the graph as keys and we initialize a list as attribute to append \n",
    "distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = {n:[] for n in nodes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the bfs_mod for each node of the input category, so we'll have in dists for each node a list with all lengths\n",
    "of  the shortest path  to that node having as starting node a node from the input category('Association_football_goalkeepers')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in cat_dic['Association_football_goalkeepers']:\n",
    "    bfs_mod(g,article,dists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check if in some cases there was no link between one node of the input categories and one of the other \n",
    "categories, we simply add 100(a number greater of the found shortest paths lengths) to the list of distances \n",
    "in that case. With this procedure we don't keep track of what source node wasn't connected to the target node, \n",
    "but since our goal is the reasearch of the median this information is not important. This is done to have an effect on the calculation of the median and so missing paths will have a crucial role to determine the ranking block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in dists.keys():\n",
    "    #searching if in some cases there was no path comparing the expected length of of the list with the real length\n",
    "    if len(dists[article])<len(cat_dic[\"Association_football_goalkeepers\"]):\n",
    "                    #adding 100 for each mssing value to the list of distanes of the article\n",
    "                    l1=[100]*(len(cat_dic['Association_football_goalkeepers'])-len(dists[article]))\n",
    "                    dists[article].extend(l1)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we calculate the distance of each category from the input category using the median of the distances of the shortest paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we create a list medians where we'll save the block ranking with the value of the median for each block\n",
    "medians = []\n",
    "for k in cat_dic.keys():\n",
    "    if k != 'Association_football_goalkeepers':\n",
    "            d = []\n",
    "            #retrieving distances for each article of the category\n",
    "            for article in cat_dic[k]:\n",
    "                  d.extend(dists[article])\n",
    "            #calculating the median\n",
    "            med = np.median(d)\n",
    "            #finally appending the values to medians\n",
    "            medians.append([k,med])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Association_football_goalkeepers', 0],\n",
       " ['British_films', 6.0],\n",
       " ['English-language_films', 6.0],\n",
       " ['American_films', 6.0],\n",
       " ['American_television_actors', 6.0],\n",
       " ['American_film_actors', 6.0],\n",
       " ['English_footballers', 7.0],\n",
       " ['The_Football_League_players', 7.0],\n",
       " ['Members_of_the_United_Kingdom_Parliament_for_English_constituencies', 7.0],\n",
       " ['Indian_films', 7.0],\n",
       " ['Rivers_of_Romania', 7.0],\n",
       " ['English-language_albums', 7.0],\n",
       " ['People_from_New_York_City', 7.0],\n",
       " ['Debut_albums', 7.0],\n",
       " ['Black-and-white_films', 7.0],\n",
       " ['American_military_personnel_of_World_War_II', 7.0],\n",
       " ['Association_football_forwards', 8.0],\n",
       " ['Harvard_University_alumni', 8.0],\n",
       " ['Major_League_Baseball_pitchers', 8.0],\n",
       " ['Year_of_birth_missing_(living_people)', 8.0],\n",
       " ['Place_of_birth_missing_(living_people)', 8.0],\n",
       " ['Windows_games', 8.0],\n",
       " ['Association_football_midfielders', 100.0],\n",
       " ['Association_football_defenders', 100.0],\n",
       " ['Year_of_death_missing', 100.0],\n",
       " ['Main_Belt_asteroids', 100.0],\n",
       " ['Asteroids_named_for_people', 100.0],\n",
       " ['Year_of_birth_missing', 100.0],\n",
       " ['Living_people', 8.0]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#showing unordered medians\n",
    "medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we insert in position 0 of the ranking block the input category with median value 0\n",
    "medians.insert(0,['Association_football_goalkeepers',0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we sort the block according to the median since medians is unordered\n",
    "medians.sort(key = lambda x : x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Association_football_goalkeepers',\n",
       " 'British_films',\n",
       " 'English-language_films',\n",
       " 'American_films',\n",
       " 'American_television_actors',\n",
       " 'American_film_actors',\n",
       " 'English_footballers',\n",
       " 'The_Football_League_players',\n",
       " 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies',\n",
       " 'Indian_films',\n",
       " 'Rivers_of_Romania',\n",
       " 'English-language_albums',\n",
       " 'People_from_New_York_City',\n",
       " 'Debut_albums',\n",
       " 'Black-and-white_films',\n",
       " 'American_military_personnel_of_World_War_II',\n",
       " 'Association_football_forwards',\n",
       " 'Harvard_University_alumni',\n",
       " 'Major_League_Baseball_pitchers',\n",
       " 'Year_of_birth_missing_(living_people)',\n",
       " 'Place_of_birth_missing_(living_people)',\n",
       " 'Windows_games',\n",
       " 'Living_people',\n",
       " 'Association_football_midfielders',\n",
       " 'Association_football_defenders',\n",
       " 'Year_of_death_missing',\n",
       " 'Main_Belt_asteroids',\n",
       " 'Asteroids_named_for_people',\n",
       " 'Year_of_birth_missing']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating and showing the ranking block without medians values\n",
    "ranking_block= []\n",
    "for m in medians:\n",
    "    ranking_block.append(m[0])\n",
    "ranking_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving medians with pickle\n",
    "with open('medians', 'wb') as f:\n",
    "        pickle.dump(medians, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving dists with pickle\n",
    "with open('dists.pickle', 'wb') as f:\n",
    "    pickle.dump(dists, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2 part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First two cells are naturally unnecessary if we run them before in the first part, but we put them here also for completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create two lists edges and n, one for edges and the last to save al nodes that we find in edges\n",
    "edges = []\n",
    "n =[]\n",
    "#we import data that we need and we use them to fill previous initialized lists \n",
    "with open(\"wiki-topcats-reduced.txt\\wiki-topcats-reduced.txt\") as mytxt:\n",
    "    for line in mytxt:\n",
    "        #splitting edge to get the two nodes that form it\n",
    "        i,j= line.split()\n",
    "        #adding the nodes to n list\n",
    "        n.extend((int(i),int(j)))\n",
    "        #adding the edge to edges list\n",
    "        edges.append([int(i),int(j)])\n",
    "#removing duplicates from n\n",
    "nodes = set(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create a dictionary with articles for each category, we take only articles in the reduced version of the graph \n",
    "#and only categories with more than 3500 articles\n",
    "cat_dic = {}\n",
    "with open (\"wiki-topcats-categories.txt\")as dc:\n",
    "    for line in dc:\n",
    "        #initializing set to save articles of categories that are in the graph\n",
    "        art1 = set([])\n",
    "        #eliminating from the line  the unuseful part\n",
    "        l1 = line.replace(\"Category:\",\"\")\n",
    "        #splitting name of the category from its articles and trnsforming them in int\n",
    "        l2 = l1.split(\";\")\n",
    "        k = l2[0]\n",
    "        art = list(map(int,l2[1].split()))\n",
    "        #checking if articles are in our graph\n",
    "        for i in art :\n",
    "            if i in nodes:\n",
    "                art1.add(i) \n",
    "        #checking if the category meets the requirement and saving it into the dictionary\n",
    "        if len(art1)>3500:\n",
    "             cat_dic[k] = art1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We eliminate duplicates articles from categories that are in a more distant position from the input category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalizing assigned set , used to keep track of yet assigned nodes\n",
    "assigned = set([])\n",
    "for category in medians:\n",
    "    #initialising a list to save not already added articles for the \"c\" category\n",
    "    usab_art = []\n",
    "    for article in cat_dic[category[0]]:\n",
    "        if article not in assigned:\n",
    "            # for each article of the category,if needed, adding it to usab_art and assigned\n",
    "            usab_art.append(article)\n",
    "            assigned.add(article)\n",
    "    #updating cat_dic\n",
    "    cat_dic[category[0]]= usab_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create a new G graph we networkx\n",
    "G=nx.DiGraph()\n",
    "#we add nodes and edges to the graph\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary to save the scores, keys of this dictionary will be the categories and this will be a nested\n",
    "dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_w ={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each category we create a subgraph with its nodes,and we save them in a dictionary that we store in cat_w, this are not already the scores that we need because they are simply the sum of the in edges form the nodes of the same category(we use the length of the in edges, found using in_edges function from networkX). We need to add also the  scores of in_edges from the precedent category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in medians:\n",
    "    #creating subgraph\n",
    "    s_g = G.subgraph(cat_dic[category[0]])\n",
    "    #temporary dictionary to store results\n",
    "    d={}\n",
    "    #adding 1 for each in edge of the same category\n",
    "    for node in s_g:\n",
    "         d[node]=len(s_g.in_edges(node))\n",
    "    cat_w[c[0]]=d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add to score of the single node the weights of the predecessors if they are in the previous categgory, to do that we use the predecessors function from networkX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating scores using the order of the ranking_block,starting from the second category\n",
    "for i in range(1,len(medians)):\n",
    "    for article in cat_w[medians[i][0]].keys():\n",
    "        #creating predecessors\n",
    "        predecessors = G.predecessors(article)\n",
    "        for predecessor in predecessors:\n",
    "            #if we find a predecessor in the previous category we add its score to the article weight to have \n",
    "            #the final score\n",
    "            if p in cat_w[medians[i-1][0]].keys():\n",
    "                cat_w[medians[i][0]][article]= cat_w[medians[i][0]][article]+cat_w[medians[i-1][0]][predecessor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create a list from articles ids(ord_list), retrieving from the dictionary cat_w the scores and the articles for each category, then we sort them and we add them to the list, we do this using the order of medians, so we have the ranking block's order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the list where to insert all ordered articles \n",
    "ord_list =[]\n",
    "#for every category in the ranking block(we use medians as before)\n",
    "for category in medians:\n",
    "    #initializing a list\n",
    "    l =[]\n",
    "    for article in cat_w[category[0]]:\n",
    "        #appending to l a list with articole and its score\n",
    "        l.append([article, cat_w[category[0]][article]])\n",
    "    #sorting the list    \n",
    "    l.sort(key = lambda x :x[1], reverse=True)\n",
    "    #adding to the final list of all articles\n",
    "    ord_list.extend(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we save ord_list with pickle\n",
    "with open(r\"ordinated_articles.pickle\", \"wb\") as ord_:\n",
    "           pickle.dump(ord_list, ord_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81952\n",
      "81854\n",
      "88889\n",
      "89871\n",
      "82053\n",
      "82589\n",
      "83527\n",
      "82477\n",
      "82631\n",
      "83514\n",
      "82736\n",
      "83722\n",
      "83985\n",
      "81945\n",
      "82422\n",
      "82049\n",
      "82621\n",
      "82758\n",
      "89712\n",
      "81953\n",
      "88951\n",
      "81896\n",
      "83536\n",
      "88866\n",
      "82541\n",
      "82832\n",
      "84393\n",
      "86080\n",
      "1005156\n",
      "1005198\n",
      "81297\n",
      "81300\n",
      "82598\n",
      "82771\n",
      "87713\n",
      "87719\n",
      "1358603\n",
      "88993\n",
      "83250\n",
      "1358682\n",
      "88948\n",
      "82620\n",
      "107564\n",
      "1361043\n",
      "83753\n",
      "88636\n",
      "88986\n",
      "88987\n",
      "81616\n",
      "81732\n",
      "82380\n",
      "83500\n",
      "83540\n",
      "84019\n",
      "86225\n",
      "88884\n",
      "88933\n",
      "88972\n",
      "89984\n",
      "737907\n",
      "82763\n",
      "84237\n",
      "895705\n",
      "86410\n",
      "88325\n",
      "88794\n",
      "81000\n",
      "81097\n",
      "81104\n",
      "81519\n",
      "81745\n",
      "81764\n",
      "82468\n",
      "82732\n",
      "82824\n",
      "82910\n",
      "83720\n",
      "83848\n",
      "84005\n",
      "86520\n",
      "1381405\n",
      "87761\n",
      "88690\n",
      "88748\n",
      "88897\n",
      "88975\n",
      "1358768\n",
      "1358805\n",
      "97305\n",
      "81321\n",
      "81456\n",
      "89699\n",
      "81578\n",
      "82793\n",
      "82831\n",
      "82933\n",
      "83067\n",
      "83422\n",
      "83596\n",
      "83606\n"
     ]
    }
   ],
   "source": [
    "#printing the first 100 elements of the list as example\n",
    "for i in range(100):\n",
    "    print(ord_list[i][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
