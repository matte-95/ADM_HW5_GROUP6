{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "n =[]\n",
    "#we import data that we need and we fill them \n",
    "with open(\"wiki-topcats-reduced.txt\\wiki-topcats-reduced.txt\") as mytxt:\n",
    "    for line in mytxt:\n",
    "        i,j= line.split()\n",
    "        n.extend((int(i),int(j)))\n",
    "        edges.append([int(i),int(j)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we eliminate duplicates from nodes\n",
    "nodes = set(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create a dictionary with articoles for each category, we take only articles in the reduced version of the graph \n",
    "#and only categories with more than 3500 articoles\n",
    "cat_dic = {}\n",
    "with open (\"wiki-topcats-categories.txt\")as dc:\n",
    "    for line in dc:\n",
    "        l1 = line.replace(\"Category:\",\"\")\n",
    "        l2 = l1.split(\";\")\n",
    "        k = l2[0]\n",
    "        art = list(map(int,l2[1].split()))\n",
    "        for i in art :\n",
    "            if i not in nodes:\n",
    "                art.remove(i) \n",
    "        if len(art)>3500:\n",
    "             cat_dic[k] = art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we store the graph in a dictionary saving from each source node a list with end nodes of its edges\n",
    "from collections import defaultdict\n",
    "g = defaultdict(list)\n",
    "for e in edges:\n",
    "    g[e[0]].append(e[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we create the bfs function, in this version the function save distances from the starting node to a set of goal \n",
    "#nodes in a dictionary\n",
    "from collections import deque\n",
    "def bfs(graph, start, goal,dists):\n",
    "    visited = set([start])\n",
    "    queue = deque([(start, [])])\n",
    "    goal1 = set(goal)\n",
    "    while queue:\n",
    "        current, path = queue.popleft()\n",
    "        visited.add(current)\n",
    "        for neighbor in graph[current]:\n",
    "            if neighbor in goal1:\n",
    "                dists[neighbor].append(len(path + [current, neighbor])-1)\n",
    "                goal1.remove(neighbor)\n",
    "            if neighbor in visited:\n",
    "                continue\n",
    "            queue.append((neighbor, path + [current]))\n",
    "            visited.add(neighbor)   \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we define the dictionary with distances\n",
    "dists = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create a list with all nodes from all categories except the input category\n",
    "int_nodes=[]\n",
    "for k in cat_dic.keys():\n",
    "    if k != 'Association_football_goalkeepers':\n",
    "            l1 = cat_dic[k]\n",
    "            l1 = [a for a in l1 if a not in cat_dic['Association_football_goalkeepers']]\n",
    "    int_nodes.extend(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we execute the bfs for each node of the input category, so we'll have in dists for each node a list with all lengths\n",
    "#of shortest path  to that node having as starting node a node from the input category\n",
    "for a in cat_dic['Association_football_goalkeepers']:\n",
    "    bfs(g,a,int_nodes,dists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we check if in some cases there was no link between one node of the input categories and one of the other \n",
    "#categories, we simply add 200000 to the list of distances in that case. With this procedure we don't keep track of what\n",
    "#node wasn't connected to the target node, but since our goal is the reasearch of the median this information is not \n",
    "#important\n",
    "for a in dists:\n",
    "    if len(dists[a])<len(\"Association_football_goalkeepers\"):\n",
    "        l1=[200000]*(len(cat_dic['Association_football_goalkeepers']-len(dists[a]))\n",
    "        dists[a].extend(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as st\n",
    "medians = []\n",
    "for k in cat_dic.keys():\n",
    "    if k != 'Association_football_goalkeepers':\n",
    "            d = []\n",
    "            for a in cat_dic[k]:\n",
    "                  d.extend(dists[a])\n",
    "            med = st.median(d)\n",
    "            medians.append([k,med])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['English_footballers', 7.0],\n",
       " ['The_Football_League_players', 7],\n",
       " ['Association_football_forwards', 7],\n",
       " ['Association_football_goalkeepers', 7.0],\n",
       " ['Association_football_midfielders', 7.0],\n",
       " ['Association_football_defenders', 7],\n",
       " ['Living_people', 6.0],\n",
       " ['Harvard_University_alumni', 5],\n",
       " ['Major_League_Baseball_pitchers', 6.0],\n",
       " ['Members_of_the_United_Kingdom_Parliament_for_English_constituencies', 6],\n",
       " ['Indian_films', 6.0],\n",
       " ['Year_of_death_missing', 200000.0],\n",
       " ['Year_of_birth_missing_(living_people)', 6.0],\n",
       " ['Rivers_of_Romania', 7.0],\n",
       " ['Main_Belt_asteroids', 200000.0],\n",
       " ['Asteroids_named_for_people', 11],\n",
       " ['English-language_albums', 5],\n",
       " ['British_films', 5],\n",
       " ['English-language_films', 4],\n",
       " ['American_films', 4],\n",
       " ['People_from_New_York_City', 5.0],\n",
       " ['American_television_actors', 200000.0],\n",
       " ['Debut_albums', 6.0],\n",
       " ['Black-and-white_films', 5.0],\n",
       " ['Year_of_birth_missing', 13],\n",
       " ['Place_of_birth_missing_(living_people)', 6],\n",
       " ['American_military_personnel_of_World_War_II', 5],\n",
       " ['Windows_games', 6]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we insert in position 0 of the ranking block the input category\n",
    "medians.insert(0,['Association_football_goalkeepers',0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we sort the block according to the median\n",
    "medians.sort(key = lambda x : x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['American_film_actors', 0],\n",
       " ['English-language_films', 4],\n",
       " ['American_films', 4],\n",
       " ['Harvard_University_alumni', 5],\n",
       " ['English-language_albums', 5],\n",
       " ['British_films', 5],\n",
       " ['People_from_New_York_City', 5.0],\n",
       " ['Black-and-white_films', 5.0],\n",
       " ['American_military_personnel_of_World_War_II', 5],\n",
       " ['Living_people', 6.0],\n",
       " ['Major_League_Baseball_pitchers', 6.0],\n",
       " ['Members_of_the_United_Kingdom_Parliament_for_English_constituencies', 6],\n",
       " ['Indian_films', 6.0],\n",
       " ['Year_of_birth_missing_(living_people)', 6.0],\n",
       " ['Debut_albums', 6.0],\n",
       " ['Place_of_birth_missing_(living_people)', 6],\n",
       " ['Windows_games', 6],\n",
       " ['English_footballers', 7.0],\n",
       " ['The_Football_League_players', 7],\n",
       " ['Association_football_forwards', 7],\n",
       " ['Association_football_goalkeepers', 7.0],\n",
       " ['Association_football_midfielders', 7.0],\n",
       " ['Association_football_defenders', 7],\n",
       " ['Rivers_of_Romania', 7.0],\n",
       " ['Asteroids_named_for_people', 11],\n",
       " ['Year_of_birth_missing', 13],\n",
       " ['Year_of_death_missing', 200000.0],\n",
       " ['Main_Belt_asteroids', 200000.0],\n",
       " ['American_television_actors', 200000.0]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving medians with pickle\n",
    "import pickle\n",
    "with open('medians', 'wb') as f:\n",
    "        pickle.dump(medians, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving dists with pickle\n",
    "with open('dists.pickle', 'wb') as f:\n",
    "    pickle.dump(dists, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2 part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_n =  defaultdict(list)\n",
    "for e in edges:\n",
    "    end_n[e[1]].append(e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_score = {}\n",
    "for a in cat_dic['American_film_actors']:\n",
    "      dic_score[a]=0\n",
    "for a in int_nodes:\n",
    "      dic_score[a]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in cat_dic['American_film_actors']:\n",
    "    summa = 0\n",
    "    for ar in end_n[a]:\n",
    "        if ar in cat_dic[\"American_film_actors\"]:\n",
    "            summa = summa + 1\n",
    "    dic_score[a]= summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-33f5e983d89d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m                         \u001b[0msumma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msumma\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdic_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[1;32melif\u001b[0m \u001b[0mar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcat_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmedians\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                         \u001b[0msumma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msumma\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mdic_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msumma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mvisited\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmedians\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "visited = list(cat_dic['American_film_actors'])\n",
    "for i in range(1,29):\n",
    "    for a in cat_dic[medians[i][0]]:\n",
    "            summa = 0\n",
    "            for ar in end_n[a]:\n",
    "                    if ar in visited:\n",
    "                        summa = summa + dic_score[ar]  \n",
    "                    elif ar in cat_dic[medians[i][0]]:\n",
    "                        summa = summa +1\n",
    "            dic_score[a]=summa  \n",
    "    visited.extend(cat_dic[medians[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_art = []\n",
    "for i in range(29):\n",
    "    l = []\n",
    "    for a in cat_dic[medians[i][0]]:\n",
    "          l.append([a,dic_score[a]])\n",
    "    l = l.sort(key = lambda x : x[1])\n",
    "    ord_art.append([medians[i][0],l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in cat_dic[\"English-language_films\"]:\n",
    "    summa = 0\n",
    "    for ar in end_n[a]:\n",
    "        if ar in cat_dic['American_film_actors']:\n",
    "            summa = summa + dic_score[ar]\n",
    "        if ar in cat_dic[\"English-language_films\"]:\n",
    "            summa = summa +1\n",
    "    dic_score[a]= summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,29):\n",
    "    for ar in end_n[a]:\n",
    "            if ar in visited:\n",
    "                 summa = summa + dic_score[ar]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "edges = []\n",
    "n =[]\n",
    "#we import data that we need and we fill them \n",
    "with open(\"wiki-topcats-reduced.txt\\wiki-topcats-reduced.txt\") as mytxt:\n",
    "    for line in mytxt:\n",
    "        i,j= line.split()\n",
    "        n.extend((int(i),int(j)))\n",
    "        edges.append((int(i),int(j)))\n",
    "nodes = set(n)\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(29):\n",
    "    l = []\n",
    "    l = l.extend(cat_dic[medians[i][0]])\n",
    "    s = G.subgraph(l)\n",
    "    for n in s:\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
